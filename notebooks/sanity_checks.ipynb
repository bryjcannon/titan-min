{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titan-Min Sanity Checks\n",
    "\n",
    "This notebook contains quick smoke tests to verify that all core components of the Titan-Min repository work correctly.\n",
    "\n",
    "## Tests Covered:\n",
    "1. **Dataset Construction**: Validate NEEDLE placement and sample properties\n",
    "2. **Model Construction**: Test forward pass shapes and ablation variants\n",
    "3. **Training Step**: Ensure loss computation and gradient flow work\n",
    "4. **End-to-End**: Complete training/validation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports and path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our modules\n",
    "from data.niah import NIAHDataset, collate\n",
    "from models.titan_min import TitanClassifier\n",
    "from models.heads import position_logits\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Construction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset construction\n",
    "print(\"üîç Testing dataset construction...\")\n",
    "\n",
    "dataset = NIAHDataset(n_samples=100, seed=42)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Test sample properties\n",
    "for i in range(5):\n",
    "    tokens, needle_pos, length = dataset[i]\n",
    "    \n",
    "    # Check needle position is valid\n",
    "    assert 0 <= needle_pos < length, f\"Invalid needle position: {needle_pos} not in [0, {length-1}]\"\n",
    "    \n",
    "    # Check exactly one needle\n",
    "    needle_count = (tokens[:length] == 127).sum().item()\n",
    "    assert needle_count == 1, f\"Expected 1 needle, found {needle_count}\"\n",
    "    \n",
    "    # Check needle is at correct position\n",
    "    actual_pos = torch.where(tokens[:length] == 127)[0].item()\n",
    "    assert actual_pos == needle_pos, f\"Needle at {actual_pos}, expected {needle_pos}\"\n",
    "    \n",
    "    print(f\"Sample {i}: length={length}, needle_pos={needle_pos} ‚úì\")\n",
    "\n",
    "print(\"‚úÖ Dataset construction tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test collate function\n",
    "print(\"üîç Testing collate function...\")\n",
    "\n",
    "batch_samples = [dataset[i] for i in range(5)]\n",
    "batch_tokens, batch_needle_pos, batch_lengths = collate(batch_samples)\n",
    "\n",
    "print(f\"Batch shapes: tokens={batch_tokens.shape}, positions={len(batch_needle_pos)}, lengths={len(batch_lengths)}\")\n",
    "\n",
    "# Verify left-padding\n",
    "for i in range(5):\n",
    "    length = batch_lengths[i]\n",
    "    max_len = batch_tokens.shape[1]\n",
    "    \n",
    "    if length < max_len:\n",
    "        pad_tokens = batch_tokens[i, :max_len-length]\n",
    "        assert (pad_tokens == 0).all(), \"Left padding should be zeros\"\n",
    "    \n",
    "    print(f\"Sample {i}: length={length}, padded_length={max_len} ‚úì\")\n",
    "\n",
    "print(\"‚úÖ Collate function tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model construction and forward pass\n",
    "print(\"üîç Testing model construction...\")\n",
    "\n",
    "model = TitanClassifier(\n",
    "    vocab_size=128,\n",
    "    dim=64,  # Small for testing\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    n_mem=2\n",
    ")\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 2, 32\n",
    "x = torch.randint(0, 128, (batch_size, seq_len))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    h_tokens, rep = model(x)\n",
    "\n",
    "print(f\"Forward pass shapes: tokens={h_tokens.shape}, rep={rep.shape}\")\n",
    "\n",
    "# Assert expected shapes\n",
    "assert h_tokens.shape == (batch_size, seq_len, 64), f\"Wrong token shape: {h_tokens.shape}\"\n",
    "assert rep.shape == (batch_size, 64), f\"Wrong rep shape: {rep.shape}\"\n",
    "\n",
    "print(\"‚úÖ Model construction tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test position logits and masking\n",
    "print(\"üîç Testing position logits...\")\n",
    "\n",
    "lengths = torch.tensor([20, 25])\n",
    "logits = position_logits(rep, h_tokens, lengths)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "assert logits.shape == (batch_size, seq_len), f\"Wrong logits shape: {logits.shape}\"\n",
    "\n",
    "# Check masking\n",
    "for i in range(batch_size):\n",
    "    length = lengths[i]\n",
    "    \n",
    "    # Positions >= length should be -inf\n",
    "    masked = logits[i, length:]\n",
    "    assert torch.all(torch.isinf(masked) & (masked < 0)), \"Masked positions should be -inf\"\n",
    "    \n",
    "    # Valid positions should be finite\n",
    "    valid = logits[i, :length]\n",
    "    assert torch.all(torch.isfinite(valid)), \"Valid positions should be finite\"\n",
    "    \n",
    "    print(f\"Sample {i}: length={length}, valid_logits={valid.shape[0]}, masked={masked.shape[0]} ‚úì\")\n",
    "\n",
    "print(\"‚úÖ Position logits tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ablation variants\n",
    "print(\"üîç Testing ablation variants...\")\n",
    "\n",
    "ablation_configs = [\n",
    "    {\"no_memory\": True},\n",
    "    {\"no_dsconv\": True},\n",
    "    {\"no_l2\": True},\n",
    "    {\"activation\": \"relu\"},\n",
    "    {\"no_memory\": True, \"no_dsconv\": True, \"no_l2\": True, \"activation\": \"relu\"}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(ablation_configs):\n",
    "    model_abl = TitanClassifier(\n",
    "        vocab_size=128, dim=64, n_heads=4, n_layers=2, n_mem=2, **config\n",
    "    )\n",
    "    \n",
    "    model_abl.eval()\n",
    "    with torch.no_grad():\n",
    "        h_tokens_abl, rep_abl = model_abl(x)\n",
    "    \n",
    "    assert h_tokens_abl.shape == (batch_size, seq_len, 64), f\"Ablation {i}: wrong shape\"\n",
    "    assert rep_abl.shape == (batch_size, 64), f\"Ablation {i}: wrong shape\"\n",
    "    \n",
    "    print(f\"Ablation {i} ({config}): ‚úì\")\n",
    "\n",
    "print(\"‚úÖ Ablation tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Step Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training step\n",
    "print(\"üîç Testing training step...\")\n",
    "\n",
    "model = TitanClassifier(vocab_size=128, dim=64, n_heads=4, n_layers=2, n_mem=2)\n",
    "dataset = NIAHDataset(n_samples=50, seed=42)\n",
    "dataloader = DataLoader(dataset, batch_size=8, collate_fn=collate)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Get a batch\n",
    "batch_tokens, batch_needle_pos, batch_lengths = next(iter(dataloader))\n",
    "print(f\"Batch loaded: {batch_tokens.shape}\")\n",
    "\n",
    "# Run training steps\n",
    "losses = []\n",
    "for step in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    h_tokens, rep = model(batch_tokens)\n",
    "    logits = position_logits(rep, h_tokens, batch_lengths)\n",
    "    loss = criterion(logits, batch_needle_pos)\n",
    "    \n",
    "    # Assertions\n",
    "    assert torch.isfinite(loss), f\"Loss not finite: {loss}\"\n",
    "    assert loss.item() > 0, f\"Loss should be positive: {loss.item()}\"\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradients\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            assert torch.isfinite(torch.tensor(grad_norm)), f\"Infinite gradient in {name}\"\n",
    "            grad_norms.append(grad_norm)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Step {step}: loss={loss.item():.4f}, avg_grad_norm={np.mean(grad_norms):.4f}\")\n",
    "\n",
    "# Check loss trend (not strict)\n",
    "if len(losses) >= 3:\n",
    "    early_avg = np.mean(losses[:2])\n",
    "    late_avg = np.mean(losses[-2:])\n",
    "    print(f\"Loss trend: {early_avg:.4f} ‚Üí {late_avg:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training step tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. End-to-End Workflow Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete training/validation workflow\n",
    "print(\"üîç Testing end-to-end workflow...\")\n",
    "\n",
    "# Create small dataset\n",
    "dataset = NIAHDataset(n_samples=32, seed=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(24))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(24, 32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=collate)\n",
    "\n",
    "# Create model\n",
    "model = TitanClassifier(vocab_size=128, dim=32, n_heads=2, n_layers=1, n_mem=1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Setup: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "train_losses = []\n",
    "\n",
    "for batch_tokens, batch_needle_pos, batch_lengths in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    h_tokens, rep = model(batch_tokens)\n",
    "    logits = position_logits(rep, h_tokens, batch_lengths)\n",
    "    loss = criterion(logits, batch_needle_pos)\n",
    "    \n",
    "    assert torch.isfinite(loss), \"Training loss not finite\"\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "print(f\"Training: avg_loss={avg_train_loss:.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_losses = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_needle_pos, batch_lengths in val_loader:\n",
    "        h_tokens, rep = model(batch_tokens)\n",
    "        logits = position_logits(rep, h_tokens, batch_lengths)\n",
    "        loss = criterion(logits, batch_needle_pos)\n",
    "        \n",
    "        assert torch.isfinite(loss), \"Validation loss not finite\"\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct += (predictions == batch_needle_pos).sum().item()\n",
    "        total += batch_needle_pos.size(0)\n",
    "\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "print(f\"Validation: avg_loss={avg_val_loss:.4f}, accuracy={accuracy:.4f}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert 0.0 <= accuracy <= 1.0, f\"Invalid accuracy: {accuracy}\"\n",
    "assert avg_val_loss > 0, f\"Invalid validation loss: {avg_val_loss}\"\n",
    "\n",
    "print(\"‚úÖ End-to-end workflow tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all cells above run without assertion errors, the Titan-Min repository is working correctly!\n",
    "\n",
    "**Tests Completed:**\n",
    "- ‚úÖ Dataset construction and NEEDLE validation\n",
    "- ‚úÖ Model forward pass and shape validation\n",
    "- ‚úÖ Position logits and masking\n",
    "- ‚úÖ Ablation variant compatibility\n",
    "- ‚úÖ Training step with finite loss and gradients\n",
    "- ‚úÖ End-to-end training/validation workflow\n",
    "\n",
    "The repository is ready for full-scale experiments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
